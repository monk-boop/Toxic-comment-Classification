{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiSSQrRnhLeW",
        "colab_type": "text"
      },
      "source": [
        "Fasttext Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7g1zuvW6d40",
        "colab_type": "code",
        "outputId": "06389d29-d74e-4c07-86c0-01f55d429dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget --header=\"Host: dl.fbaipublicfiles.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://fasttext.cc/docs/en/english-vectors.html\" --header=\"Cookie: __cfduid=d9bce14e3ad5d42528e65276ed8866b581579087931\" --header=\"Connection: keep-alive\" \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\" -O \"crawl-300d-2M.vec.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-19 08:52:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  26.3MB/s    in 56s     \n",
            "\n",
            "2020-01-19 08:53:12 (26.0 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTlcgjvfhPC4",
        "colab_type": "text"
      },
      "source": [
        "Twitter download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxru8Knw-Wb6",
        "colab_type": "code",
        "outputId": "1a8fdc9e-c34d-4a8b-8a38-059358bd8b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: downloads.cs.stanford.edu\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: _ga=GA1.2.2034969121.1579138171; _gid=GA1.2.1181918881.1579138171; _gat=1\" --header=\"Connection: keep-alive\" \"http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\" -O \"glove.twitter.27B.zip\" -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-19 08:53:14--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  2.05MB/s    in 11m 44s \n",
            "\n",
            "2020-01-19 09:04:58 (2.06 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6t01gMdhRtu",
        "colab_type": "text"
      },
      "source": [
        "Importing and extracting zip files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xukl2xjy6vm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/crawl-300d-2M.vec.zip', 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42u-gyw2-nJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/glove.twitter.27B.zip', 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0TujPT3c91",
        "colab_type": "code",
        "outputId": "9e4553ce-40dc-488b-ba4a-ccdd3fc3de0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT2AJrUwhfIP",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC7EA3w861n1",
        "colab_type": "code",
        "outputId": "5db92000-50dd-4675-e6c2-17fefd790fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%tensorflow_version 1.x\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec \n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "from PIL import Image\n",
        "import matplotlib_venn as venn\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding,Dropout,Activation,GRU,Conv1D,CuDNNGRU,CuDNNLSTM\n",
        "from keras.layers import SpatialDropout1D,MaxPool1D,GlobalAveragePooling1D,RepeatVector ,Add,PReLU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D,BatchNormalization,concatenate,TimeDistributed,Flatten\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.optimizers import Adam,SGD,Nadam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers.core import Layer  \n",
        "from keras import initializers, regularizers, constraints  \n",
        "from keras import backend as K\n",
        "from nltk.stem import SnowballStemmer\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ED7z7RWBP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmuRFUU7hjz6",
        "colab_type": "text"
      },
      "source": [
        "Import Train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk1F_p0m3eM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/self2/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/self2/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_18Uidj-4Qm",
        "colab_type": "text"
      },
      "source": [
        "EDA\n",
        "Update:\n",
        "The kernal has been updated for the new test and train datasets.\n",
        "\n",
        "Introduction:\n",
        "Being anonymous over the internet can sometimes make people say nasty things that they normally would not in real life. Let's filter out the hate from our platforms one comment at a time.\n",
        "\n",
        "Objective:\n",
        "To create an EDA/ feature-engineering starter notebook for toxic comment classification.\n",
        "\n",
        "Data Overview:\n",
        "The dataset here is from wiki corpus dataset which was rated by human raters for toxicity. The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015.\n",
        "\n",
        "Different platforms/sites can have different standards for their toxic screening process. Hence the comments are tagged in the following five categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASJofB0Jjpym",
        "colab_type": "text"
      },
      "source": [
        "Check the number of comments that are clean below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzH02TxZ-_F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will use this to stratify our dataset later on\n",
        "#del train['clean']\n",
        "x= train.iloc[:,2:].sum(axis = 1)\n",
        "train['clean'] = (x==0)\n",
        "train['clean'] = train['clean'].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjLj_RS0FnU-",
        "colab_type": "code",
        "outputId": "dc8e846f-2e72-4f1a-e0fb-aafff367b844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "x=train.iloc[:,2:].sum()\n",
        "#plot\n",
        "plt.figure(figsize=(8,4))\n",
        "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"# per class\")\n",
        "plt.ylabel('# of Occurrences', fontsize=12)\n",
        "plt.xlabel('Type ', fontsize=12)\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAEaCAYAAABw/39TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PM-VHZEK66j",
        "colab_type": "code",
        "outputId": "3b479f84-7f3a-4206-f200-37b57ec40eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"total number of clean comments is,\",train['clean'].sum(axis = 0),\"as percentage is \",float((train['clean'].sum(axis = 0))/train.shape[0]*100), \"%\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of clean comments is, 143346 as percentage is  89.83211235124176 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHT2I0b78B_I",
        "colab_type": "code",
        "outputId": "f07f7874-bb4f-4180-8b6a-2493ed105bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train.isnull().any()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id               False\n",
              "comment_text     False\n",
              "toxic            False\n",
              "severe_toxic     False\n",
              "obscene          False\n",
              "threat           False\n",
              "insult           False\n",
              "identity_hate    False\n",
              "clean            False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALQe1RQP-Aly",
        "colab_type": "code",
        "outputId": "9eee54b7-b1ef-4fbd-b729-e9279fd6e588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "test.isnull().any()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id              False\n",
              "comment_text    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOzMc07C976K",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF0-97iRUHnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocess(text): \n",
        "    \n",
        "    # Emoticons\n",
        "    text = text.replace(\":/\", \" bad \")\n",
        "    text = text.replace(\":&gt;\", \" sad \")\n",
        "    text = text.replace(\":')\", \" sad \")\n",
        "    text = text.replace(\":-(\", \" frown \")\n",
        "    text = text.replace(\":(\", \" frown \")\n",
        "    text = text.replace(\":s\", \" frown \")\n",
        "    text = text.replace(\":-s\", \" frown \")\n",
        "    text = text.replace(\"&lt;3\", \" heart \")\n",
        "    text = text.replace(\":d\", \" smile \")\n",
        "    text = text.replace(\":p\", \" smile \")\n",
        "    text = text.replace(\":dd\", \" smile \")\n",
        "    text = text.replace(\"8)\", \" smile \")\n",
        "    text = text.replace(\":-)\", \" smile \")\n",
        "    text = text.replace(\":)\", \" smile \")\n",
        "    text = text.replace(\";)\", \" smile \")\n",
        "    text = text.replace(\"(-:\", \" smile \")\n",
        "    text = text.replace(\"(:\", \" smile \")\n",
        "    text = text.replace(\":/\", \" worry \")\n",
        "    text = text.replace(\":&gt;\", \" angry \")\n",
        "    text = text.replace(\":')\", \" sad \")\n",
        "    text = text.replace(\":-(\", \" sad \")\n",
        "    text = text.replace(\":(\", \" sad \")\n",
        "    text = text.replace(\":s\", \" sad \")\n",
        "    text = text.replace(\":-s\", \" sad \")\n",
        "    text = text.replace(\"fu ck\", \"fuck\")\n",
        "    # Shortforms   \n",
        "    text = re.sub(r'[\\w]*don\\'t[\\w]*','do not',text)\n",
        "    text = re.sub(r'[\\w]*i\\'ll[\\w]*','i will',text)\n",
        "    text = re.sub(r'[\\w]*wasn\\'t[\\w]*','was not',text)\n",
        "    text = re.sub(r'[\\w]*there\\'s[\\w]*','there is',text)\n",
        "    text = re.sub(r'[\\w]*i\\'m[\\w]*','i am',text)\n",
        "    text = re.sub(r'[\\w]*won\\'t[\\w]*','will not',text)\n",
        "    text = re.sub(r'[\\w]*let\\'s[\\w]*','let us',text)\n",
        "    text = re.sub(r'[\\w]*i\\'d[\\w]*','i would',text)\n",
        "    text = re.sub(r'[\\w]*they\\'re[\\w]*','they are',text)\n",
        "    text = re.sub(r'[\\w]*haven\\'t[\\w]*','have not',text)\n",
        "    text = re.sub(r'[\\w]*that\\'s[\\w]*','that is',text)\n",
        "    text = re.sub(r'[\\w]*couldn\\'t[\\w]*','could not',text)\n",
        "    text = re.sub(r'[\\w]*aren\\'t[\\w]*','are not',text)\n",
        "    text = re.sub(r'[\\w]*wouldn\\'t[\\w]*','would not',text)\n",
        "    text = re.sub(r'[\\w]*you\\'ve[\\w]*','you have',text)\n",
        "    text = re.sub(r'[\\w]*you\\'ll[\\w]*','you will',text)\n",
        "    text = re.sub(r'[\\w]*what\\'s[\\w]*','what is',text)\n",
        "    text = re.sub(r'[\\w]*we\\'re[\\w]*','we are',text)\n",
        "    text = re.sub(r'[\\w]*doesn\\'t[\\w]*','does not',text)\n",
        "    text = re.sub(r'[\\w]*can\\'t[\\w]*','can not',text)\n",
        "    text = re.sub(r'[\\w]*shouldn\\'t[\\w]*','should not',text)\n",
        "    text = re.sub(r'[\\w]*didn\\'t[\\w]*','did not',text)\n",
        "    text = re.sub(r'[\\w]*here\\'s[\\w]*','here is',text)\n",
        "    text = re.sub(r'[\\w]*you\\'d[\\w]*','you would',text)\n",
        "    text = re.sub(r'[\\w]*he\\'s[\\w]*','he is',text)\n",
        "    text = re.sub(r'[\\w]*she\\'s[\\w]*','she is',text)\n",
        "    text = re.sub(r'[\\w]*weren\\'t[\\w]*','were not',text)\n",
        "    \n",
        "    \n",
        "    # Remove punct except ! and ?\n",
        "    text = re.sub(r\"[,.:|(;@)-/^—#&%$<=>`~{}\\[\\]\\'\\\"]+\\ *\", \" \", text)\n",
        "    # Separate out ! and ?\n",
        "    text = re.sub(\"!\", \" ! \", text)\n",
        "    text = re.sub(\"\\?\", \" ? \", text)\n",
        "  \n",
        "    # Drop numbers\n",
        "    text = re.sub(\"\\\\d+\", \" \", text)\n",
        "        \n",
        "    # Check if at least 3 consecutive substrings are in caps. Add <caps> tag at the end\n",
        "    counter = 0\n",
        "    for substr in text.split():\n",
        "        if (substr.isupper() == True):\n",
        "            counter += 1\n",
        "            if counter >=3:\n",
        "                text = text + \" \" + \"XYZ\" # XYZ chosen for capitals since it is a rare word present in embedding\n",
        "                counter = 0\n",
        "        else:\n",
        "            if counter >=3:\n",
        "                text = text + \" \" + \"XYZ\"\n",
        "                counter = 0\n",
        "            else:\n",
        "                counter = 0\n",
        "    \n",
        "    # Convert to lower\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Lots of words are not present in the fasttext embeddings. Replace them\n",
        "    text = re.sub(r'[\\w]*(fuc|fck|fvc|fuk|fucd)[\\w]*','fuck',text)\n",
        "    text = re.sub(r'[\\w]*fag[\\w]*','gay',text)\n",
        "    text = re.sub(r'[\\w]*gay[\\w]*','gay',text)\n",
        "    text = re.sub(r'[\\w]*peni[\\w]*','dick',text)\n",
        "    text = re.sub(r'[\\w]*(dic|dik)[\\w]*','dick',text)\n",
        "    text = re.sub(r'[\\w]*bi[\\w]*ch[\\w]*','bitch',text)\n",
        "    text = re.sub(r'[\\w]*s[\\w]*x[\\w]*','sex',text)\n",
        "    text = re.sub(r'[\\w]*s[\\w]*k[\\w]*','suck',text)\n",
        "    text = re.sub(r'[\\w]*nigg[\\w]*','suck',text)\n",
        "    text = re.sub(r'[\\w]*cock[\\w]*','dick',text)\n",
        "    text = re.sub(r'[\\w]*cunt[\\w]*','cunt',text)\n",
        "    text = re.sub(r'[\\w]*anal[\\w]*','anal',text)\n",
        "    text = re.sub(r'[\\w]*ha{2,}[\\w]*','haha',text)\n",
        "    text = re.sub(r'[\\w]*haha[\\w]*','haha',text)\n",
        "    text = re.sub(r'[\\w]*wiki[\\w]*','wikipedia',text)\n",
        "    text = re.sub(r'[\\w]*ency[\\w]ia[\\w]*','encyclopedia',text)   \n",
        "           \n",
        "    # Remove unwanted space\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ8iXQUG-D3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    \n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "sent = decontracted(train['comment_text'].values[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQERWCjoT4VV",
        "colab_type": "code",
        "outputId": "a8679d04-c2e8-4d1e-c65c-0fbdc32d5e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "However, the Moonlite edit noted by golden daph was me (on optus ...)  Wake up wikkis.  So funny\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ1DrhgeUEAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnLSizsuUP8j",
        "colab_type": "code",
        "outputId": "128f5bd8-6c9f-4799-9e14-7554cb831690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(sent)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "However the Moonlite edit noted by golden daph was me on optus Wake up wikkis So funny\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoY-YzRkURrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-KQIPZkP1i",
        "colab_type": "text"
      },
      "source": [
        "cleaning train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooK74rDEUXzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "clean_text_train = []\n",
        "#s = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
        "  \n",
        "for i in train['comment_text'].values:\n",
        "  sent = text_preprocess(i)\n",
        "  sent = re.sub('[^A-Za-z0-9]+',' ', sent)\n",
        "  \n",
        "  sent = ' '.join(e for e in sent.split() if e not in stopwords and len(e)>1)\n",
        "  clean_text_train.append(sent.lower().strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGmK42e-kSLI",
        "colab_type": "text"
      },
      "source": [
        "Cleaning test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRS3S3LYlE7W",
        "colab_type": "code",
        "outputId": "36699d3a-e4c2-497d-8d68-9fe2d1df23f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "clean_text_test = []\n",
        "for i in test['comment_text'].values:\n",
        "  sent = text_preprocess(i)\n",
        "  sent = re.sub('[^A-Za-z0-9]+',' ', sent)\n",
        "  \n",
        " \n",
        "  \n",
        "  sent = ' '.join(e for e in sent.split() if e not in stopwords and len(e)>1)\n",
        "  clean_text_test.append(sent.lower().strip())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in test[\\'comment_text\\'].values:\\n  sent = decontracted(i)\\n  sent = re.sub(\\'[^A-Za-z0-9]+\\', \\' \\', sent)\\n  sent = re.sub(\\'<[^<]+?>\\', \\'\\', sent)\\n  sent = re.sub(r\"([0-9]+)000000\", r\"\\x01m\", sent)\\n  sent = re.sub(r\"([0-9]+)000\", r\"\\x01k\", sent)\\n  sent = \\' \\'.join(e for e in sent.split() if e not in stopwords)\\n  clean_text_test.append(sent.lower().strip())'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk05VVQ6Xdzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['comment_text'] = clean_text_train\n",
        "test['comment_text'] = clean_text_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsaJoMgAnBDz",
        "colab": {}
      },
      "source": [
        "#Tried to increase vocabulary of the model. Didn't work\n",
        "'''def translate(comment, language):\n",
        "    if hasattr(comment, \"decode\"):\n",
        "        comment = comment.decode(\"utf-8\")\n",
        "\n",
        "    text = TextBlob(comment)\n",
        "    try:\n",
        "        text = text.translate(to=language)\n",
        "        text = text.translate(to=\"en\")\n",
        "    except NotTranslated:\n",
        "        pass\n",
        "\n",
        "    return str(text)\n",
        "\n",
        "def main():\n",
        "    \n",
        "\n",
        "    \n",
        "    languages = [\"es\", \"de\", \"fr\"]\n",
        "    \n",
        "    comments_list = train[\"comment_text\"].str.lower().fillna('something').values\n",
        "\n",
        "    if not os.path.exists(\"/content/drive/My Drive/\"):\n",
        "        os.mkdir(\"/content/drive/My Drive/\")\n",
        "\n",
        "    parallel = Parallel(n_jobs = -1,backend=\"threading\", verbose=5)\n",
        "    for language in languages:\n",
        "        print('Translate comments using \"{0}\" language'.format(language))\n",
        "        translated_data = parallel(delayed(translate)(comment, language) for comment in comments_list)\n",
        "        train[\"comment_text\"] = translated_data\n",
        "\n",
        "        result_path = os.path.join(args.result_path, \"train_\" + language + \".csv\")\n",
        "        train.to_csv(result_path, index=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POjfvSd5oTeH",
        "colab_type": "text"
      },
      "source": [
        "Modeling on various embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXCag3pD9BS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 100000\n",
        "maxlen = 150\n",
        "embed_size = 200\n",
        "batch_size = 128\n",
        "n_splits = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJsHZtgS9BPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
        "np.random.seed(32)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "max_features = 272330\n",
        "maxlen = 150\n",
        "embed_size = 200\n",
        "batch_size = 2048\n",
        "n_splits = 4\n",
        "\n",
        "\n",
        "# Classs for evaluating the metric\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWMY0B4HkqDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load embedding file, seperate train and test classes\n",
        "EMBEDDING_FILE='/content/glove.twitter.27B.200d.txt'\n",
        "train = pd.read_csv('/content/drive/My Drive/self2/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/self2/test.csv')\n",
        "\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "# load Clean text\n",
        "train['comment_text'] = clean_text_train\n",
        "test['comment_text'] = clean_text_test\n",
        "\n",
        "\n",
        "# Vectorize comments\n",
        "list_sentences_train = train[\"comment_text\"].str.lower().fillna(\"_na_\").values\n",
        "list_sentences_test = test[\"comment_text\"].str.lower().fillna(\"_na_\").values\n",
        "    \n",
        "tokenizer = Tokenizer(num_words = max_features, lower = True,filters='\"#$%&()*+,-./:;=@[\\\\]^_`“<>{|}~\\t\\n') # not filtering out ! and ?, < >\n",
        "# Fit on both train and test to take all the vocabulary\n",
        "# People will question why take test. it is exposing to a wider vocabulary                      \n",
        "tokenizer.fit_on_texts(list(list_sentences_train)+list(list_sentences_test))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "#padding the sequences to a maxlen of 150 per comment\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "#Load word vector\n",
        "def get_coefs(word,*arr): \n",
        "  return word, np.asarray(arr, dtype='float32')    \n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La826WhnlHfP",
        "colab_type": "code",
        "outputId": "b011e84e-afed-463d-b0d8-f05b035d7bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "len(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "272337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Oe_0dpHlIzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creare embedding matrix\n",
        "nb_words = min(max_features, len(word_index)) \n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    # we need to create a matrix for each word in the dataset - embedding matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbuNKHH6mpvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use stratified k folds to split data into train and test\n",
        "# we are using stratified k folds in htis case because train_test_split won't work\n",
        "folds = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state=25)\n",
        "oof = np.empty([len(X_t),len(list_classes)])\n",
        "sub_preds = np.zeros([len(X_te),len(list_classes)])\n",
        "foldwise_auc = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qZ8rn44oyW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv1D, MaxPool1D, BatchNormalization, Concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgvCf5k6oJjm",
        "colab_type": "code",
        "outputId": "e7f39064-c90c-43c9-af53-4e05ea0a1a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#ttps://www.kaggle.com/ogrellier/kfold-or-stratifiedkfold\n",
        "#https://www.kaggle.com/christofhenkel/inceptioncnn-with-flip\n",
        "#https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79911\n",
        "# triain the model\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y[:,0], y[:,0])): #StratifiedKFold expects array of shape (n,)\n",
        "    filter_sizes = [1,2,3,5]\n",
        "  \n",
        "    X_train, y_train = X_t[trn_idx], y[trn_idx]\n",
        "    X_val, y_val = X_t[val_idx], y[val_idx]\n",
        "    num_filters = 36    \n",
        "    print(\"Running fold %d\" % fold_)     \n",
        "        \n",
        "    ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "    earlystop = EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=1) \n",
        "        \n",
        "    def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
        "        inp = Input(shape = (maxlen,))\n",
        "        x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "        \n",
        "        x = SpatialDropout1D(dr)(x)\n",
        "        conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0]),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "        conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1]),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "        conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2]), \n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "        conv_3 = Conv1D(num_filters, kernel_size=(filter_sizes[3]),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "\n",
        "        maxpool_0 = MaxPool1D(pool_size=(maxlen - filter_sizes[0] + 1))(conv_0)\n",
        "        maxpool_1 = MaxPool1D(pool_size=(maxlen - filter_sizes[1] + 1))(conv_1)\n",
        "        maxpool_2 = MaxPool1D(pool_size=(maxlen - filter_sizes[2] + 1))(conv_2)\n",
        "        maxpool_3 = MaxPool1D(pool_size=(maxlen - filter_sizes[3] + 1))(conv_3)\n",
        "\n",
        "        z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
        "        z = Flatten()(z)\n",
        "        z = BatchNormalization()(z)\n",
        "        outp = Dense(6, activation=\"sigmoid\")(z)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "            \n",
        "        model = Model(inputs = inp, outputs = outp)\n",
        "        model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "        model.fit(X_train, y_train, batch_size = 512, epochs = 4, validation_data = (X_val, y_val), \n",
        "                                verbose = 1, callbacks = [ra_val, earlystop])\n",
        "            \n",
        "        return model\n",
        "                                         \n",
        "    model = build_model(lr = 1e-3, lr_d = 0, units = 144, dr = 0.2)\n",
        "        \n",
        "    pred = model.predict(X_val, batch_size = 1024, verbose = 1)       \n",
        "    #Save model after every epoch   \n",
        "    filename = '/content/drive/My Drive/twitter_' + str(fold_) + '.h5'   \n",
        "    model.save(filename)\n",
        "    oof[val_idx] = pred\n",
        "        \n",
        "    sub_preds += model.predict([X_te], batch_size=1024, verbose=1) / n_splits\n",
        "            \n",
        "auc=0\n",
        "for i in range(len(list_classes)):\n",
        "    auc += roc_auc_score(y[:,i], oof[:,i]) / len(list_classes)\n",
        "    \n",
        "print(\"AUC for full run: %.6f\" % auc)\n",
        "    \n",
        "validation = pd.DataFrame(oof, columns = list_classes)\n",
        "validation.to_csv('validation_fasttext_bgrucnn.csv', index=False) \n",
        "    \n",
        "submission = pd.concat([test['id'], pd.DataFrame(sub_preds, columns = list_classes)], axis=1)\n",
        "submission.to_csv('submission_fasttext_bgrucnn.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold 0\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 8s 66us/step - loss: 0.2991 - acc: 0.8788 - val_loss: 0.0673 - val_acc: 0.9800\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.945394\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0606 - acc: 0.9804 - val_loss: 0.0540 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.968186\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0513 - acc: 0.9815 - val_loss: 0.0509 - val_acc: 0.9814\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.975913\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0475 - acc: 0.9824 - val_loss: 0.0500 - val_acc: 0.9814\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.977498\n",
            "39893/39893 [==============================] - 1s 13us/step\n",
            "153164/153164 [==============================] - 2s 13us/step\n",
            "Running fold 1\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 8s 66us/step - loss: 0.3007 - acc: 0.8770 - val_loss: 0.0653 - val_acc: 0.9797\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.946645\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0603 - acc: 0.9805 - val_loss: 0.0546 - val_acc: 0.9806\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.969163\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0508 - acc: 0.9819 - val_loss: 0.0529 - val_acc: 0.9807\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.972931\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0467 - acc: 0.9827 - val_loss: 0.0515 - val_acc: 0.9811\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.976124\n",
            "39893/39893 [==============================] - 1s 13us/step\n",
            "153164/153164 [==============================] - 2s 13us/step\n",
            "Running fold 2\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 8s 67us/step - loss: 0.3166 - acc: 0.8690 - val_loss: 0.0706 - val_acc: 0.9798\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.947226\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0615 - acc: 0.9802 - val_loss: 0.0551 - val_acc: 0.9809\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.967557\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0514 - acc: 0.9815 - val_loss: 0.0530 - val_acc: 0.9811\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.974058\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 6s 54us/step - loss: 0.0477 - acc: 0.9823 - val_loss: 0.0518 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.976011\n",
            "39893/39893 [==============================] - 1s 13us/step\n",
            "153164/153164 [==============================] - 2s 13us/step\n",
            "Running fold 3\n",
            "Train on 119679 samples, validate on 39892 samples\n",
            "Epoch 1/4\n",
            "119679/119679 [==============================] - 8s 70us/step - loss: 0.3024 - acc: 0.8756 - val_loss: 0.0652 - val_acc: 0.9799\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.946414\n",
            "Epoch 2/4\n",
            "119679/119679 [==============================] - 6s 54us/step - loss: 0.0600 - acc: 0.9805 - val_loss: 0.0530 - val_acc: 0.9810\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.976501\n",
            "Epoch 3/4\n",
            "119679/119679 [==============================] - 6s 54us/step - loss: 0.0510 - acc: 0.9816 - val_loss: 0.0518 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.977336\n",
            "Epoch 4/4\n",
            "119679/119679 [==============================] - 6s 53us/step - loss: 0.0476 - acc: 0.9824 - val_loss: 0.0511 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.978705\n",
            "39892/39892 [==============================] - 1s 14us/step\n",
            "153164/153164 [==============================] - 2s 13us/step\n",
            "AUC for full run: 0.976669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fkhkW_nmxbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8T4j0zTlJiU",
        "colab_type": "text"
      },
      "source": [
        "Second model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsagNaMAYCEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# same process as above but different word embedding vectors\n",
        "EMBEDDING_FILE='/content/crawl-300d-2M.vec'\n",
        "train = pd.read_csv('/content/drive/My Drive/self2/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/self2/test.csv')\n",
        "#misspellings = pd.read_csv('misspellings.csv')\n",
        "    \n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "# Clean text\n",
        "train['comment_text'] = clean_text_train\n",
        "test['comment_text'] = clean_text_test\n",
        "\n",
        "\n",
        "# Vectorize comments\n",
        "list_sentences_train = train[\"comment_text\"].str.lower().fillna(\"_na_\").values\n",
        "list_sentences_test = test[\"comment_text\"].str.lower().fillna(\"_na_\").values\n",
        "    \n",
        "tokenizer = Tokenizer(num_words = max_features, lower = True,filters='\"#$%&()*+,-./:;=@[\\\\]^_`“<>{|}~\\t\\n') # not filtering out ! and ?, < >\n",
        "# Fit on both train and test to take all the vocabulary\n",
        "# People will question why take test. It may be worng but as far as the competition is concerned, it gives me a high score.                      \n",
        "tokenizer.fit_on_texts(list(list_sentences_train)+list(list_sentences_test))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "#padding the sequences to a maxlen of 150 per comment\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# Load word vector - Source : https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python  \n",
        "def get_coefs(word,*arr): \n",
        "  return word, np.asarray(arr, dtype='float32')    \n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFYoa39p4_QZ",
        "colab_type": "code",
        "outputId": "7da1f046-bffb-4a47-9965-04aa99302077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "len(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "272337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT5ys9tPwkns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "nb_words = min(max_features, len(word_index)) \n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "    # we need to create a matrix for each word in the dataset - embedding matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PvkWwfi0Ose",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folds = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state=25)\n",
        "oof = np.empty([len(X_t),len(list_classes)])\n",
        "sub_preds = np.zeros([len(X_te),len(list_classes)])\n",
        "foldwise_auc = []   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfpD6E47ctV",
        "colab_type": "code",
        "outputId": "3de464b9-2517-4063-ae95-9c8bf73e208e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#ttps://www.kaggle.com/ogrellier/kfold-or-stratifiedkfold\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y[:,0], y[:,0])): #StratifiedKFold expects array of shape (n,)\n",
        "        \n",
        "    X_train, y_train = X_t[trn_idx], y[trn_idx]\n",
        "    X_val, y_val = X_t[val_idx], y[val_idx]\n",
        "        \n",
        "    print(\"Running fold %d\" % fold_)     \n",
        "        \n",
        "    ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "    earlystop = EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=1) \n",
        "        \n",
        "    def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
        "        inp = Input(shape = (maxlen,))\n",
        "        x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "        \n",
        "        x = SpatialDropout1D(0.4)(x)\n",
        "        x = Bidirectional(GRU(units, return_sequences = True))(x)\n",
        "\n",
        "        conv_0 = Conv1D(80, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "        conv_1 = Conv1D(80, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "        conv_2 = Conv1D(80, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "        conv_3 = Conv1D(80, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "\n",
        "        avg_0 = GlobalAveragePooling1D()(conv_0)\n",
        "        maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
        "\n",
        "        avg_1 = GlobalAveragePooling1D()(conv_1)\n",
        "        maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
        "\n",
        "        avg_2 = GlobalAveragePooling1D()(conv_2)\n",
        "        maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
        "\n",
        "        avg_3 = GlobalAveragePooling1D()(conv_3)\n",
        "        maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
        "\n",
        "        v0_col = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n",
        "        v1_col = concatenate([avg_1, avg_2, avg_0, avg_3])\n",
        "\n",
        "        merged_tensor = concatenate([v0_col, v1_col])\n",
        "\n",
        "\n",
        "        x = Dropout(0.5)(merged_tensor)\n",
        "        x = Dense(144,activation='relu')(x)\n",
        "        x = Dense(6, activation = \"sigmoid\")(x)\n",
        "            \n",
        "        model = Model(inputs = inp, outputs = x)\n",
        "        model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "        model.fit(X_train, y_train, batch_size = 2048, epochs = 4, validation_data = (X_val, y_val), \n",
        "                                verbose = 1, callbacks = [ra_val, earlystop])\n",
        "            \n",
        "        return model\n",
        "                                         \n",
        "    model = build_model(lr = 1e-3, lr_d = 0, units = 144, dr = 0.2)\n",
        "        \n",
        "    pred = model.predict(X_val, batch_size = 1024, verbose = 1)          \n",
        "    filename = '/content/drive/My Drive/fasttext_' + str(fold_) + '.h5'   \n",
        "    model.save(filename)\n",
        "    oof[val_idx] = pred\n",
        "        \n",
        "    sub_preds += model.predict([X_te], batch_size=1024, verbose=1) / n_splits\n",
        "            \n",
        "auc=0\n",
        "for i in range(len(list_classes)):\n",
        "    auc += roc_auc_score(y[:,i], oof[:,i]) / len(list_classes)\n",
        "    \n",
        "print(\"AUC for full run: %.6f\" % auc)\n",
        "    \n",
        "validation = pd.DataFrame(oof, columns = list_classes)\n",
        "validation.to_csv('validation_fasttext_bgrucnn.csv', index=False) \n",
        "    \n",
        "submission = pd.concat([test['id'], pd.DataFrame(sub_preds, columns = list_classes)], axis=1)\n",
        "submission.to_csv('submission_fasttext_bgrucnn.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold 0\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 39s 322us/step - loss: 0.1254 - acc: 0.9589 - val_loss: 0.0573 - val_acc: 0.9789\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.963856\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 36s 299us/step - loss: 0.0574 - acc: 0.9794 - val_loss: 0.0503 - val_acc: 0.9811\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.973833\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 35s 297us/step - loss: 0.0528 - acc: 0.9806 - val_loss: 0.0483 - val_acc: 0.9818\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.976130\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 36s 297us/step - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0476 - val_acc: 0.9817\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.977882\n",
            "39893/39893 [==============================] - 6s 159us/step\n",
            "153164/153164 [==============================] - 24s 160us/step\n",
            "Running fold 1\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 39s 324us/step - loss: 0.1203 - acc: 0.9635 - val_loss: 0.0572 - val_acc: 0.9787\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.962414\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 35s 296us/step - loss: 0.0562 - acc: 0.9794 - val_loss: 0.0509 - val_acc: 0.9808\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.973147\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 35s 296us/step - loss: 0.0521 - acc: 0.9806 - val_loss: 0.0488 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.976524\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 35s 296us/step - loss: 0.0502 - acc: 0.9812 - val_loss: 0.0478 - val_acc: 0.9818\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.978537\n",
            "39893/39893 [==============================] - 6s 156us/step\n",
            "153164/153164 [==============================] - 24s 159us/step\n",
            "Running fold 2\n",
            "Train on 119678 samples, validate on 39893 samples\n",
            "Epoch 1/4\n",
            "119678/119678 [==============================] - 39s 327us/step - loss: 0.1324 - acc: 0.9577 - val_loss: 0.0591 - val_acc: 0.9792\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.958571\n",
            "Epoch 2/4\n",
            "119678/119678 [==============================] - 35s 296us/step - loss: 0.0575 - acc: 0.9792 - val_loss: 0.0519 - val_acc: 0.9809\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.971534\n",
            "Epoch 3/4\n",
            "119678/119678 [==============================] - 35s 294us/step - loss: 0.0527 - acc: 0.9803 - val_loss: 0.0499 - val_acc: 0.9813\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.974482\n",
            "Epoch 4/4\n",
            "119678/119678 [==============================] - 35s 295us/step - loss: 0.0507 - acc: 0.9809 - val_loss: 0.0490 - val_acc: 0.9815\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.976090\n",
            "39893/39893 [==============================] - 6s 156us/step\n",
            "153164/153164 [==============================] - 24s 159us/step\n",
            "Running fold 3\n",
            "Train on 119679 samples, validate on 39892 samples\n",
            "Epoch 1/4\n",
            "119679/119679 [==============================] - 40s 331us/step - loss: 0.1233 - acc: 0.9620 - val_loss: 0.0571 - val_acc: 0.9796\n",
            "\n",
            " ROC-AUC - epoch: 0 - score: 0.964718\n",
            "Epoch 2/4\n",
            "119679/119679 [==============================] - 36s 298us/step - loss: 0.0578 - acc: 0.9791 - val_loss: 0.0503 - val_acc: 0.9809\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.974542\n",
            "Epoch 3/4\n",
            "119679/119679 [==============================] - 35s 296us/step - loss: 0.0533 - acc: 0.9802 - val_loss: 0.0484 - val_acc: 0.9814\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.976969\n",
            "Epoch 4/4\n",
            "119679/119679 [==============================] - 35s 296us/step - loss: 0.0512 - acc: 0.9809 - val_loss: 0.0469 - val_acc: 0.9818\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.979082\n",
            "39892/39892 [==============================] - 6s 157us/step\n",
            "153164/153164 [==============================] - 25s 160us/step\n",
            "AUC for full run: 0.977539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcrrzbBsrXvU",
        "colab_type": "text"
      },
      "source": [
        "Make a stacking emsemble of neural nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UV24zyaLonB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8eUyGRnLYOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd09qCfxndS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_name = '/content/drive/My Drive/'\n",
        "layer_name = ['twitter_3.h5','fasttext_3.h5']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1N0HiTUnc_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All of the code below has been inspired from https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
        "for i in layer_name:\n",
        "  model = load_model(dir_name+i)\n",
        "  f=0\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "    layer.name = 'ensemble'+i+'_'+layer.name\n",
        "    f+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3m60F2mrnYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_models = []\n",
        "for i in layer_name:\n",
        "  model = load_model(dir_name+i)\n",
        "  all_models.append(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwJwGf0ZoqGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_visible = [model.input for model in all_models]\n",
        "ensemble_outputs = [model.output for model  in all_models]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGiSFjU9QTMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state=25)\n",
        "oof = np.empty([len(X_t),len(list_classes)])\n",
        "sub_preds = np.zeros([len(X_te),len(list_classes)])\n",
        "foldwise_auc = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6bOVl03MUWt",
        "colab_type": "code",
        "outputId": "bc6a83dc-c6ae-4baf-ece9-6eba96604faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y[:,0], y[:,0])): #StratifiedKFold expects array of shape (n,)\n",
        "    if fold_ == 1:\n",
        "      break   \n",
        "    X_train, y_train = X_t[trn_idx], y[trn_idx]\n",
        "    X_val, y_val = X_t[val_idx], y[val_idx]\n",
        "        \n",
        "    print(\"Running fold %d\" % fold_)     \n",
        "        \n",
        "    ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "    earlystop = EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=1) \n",
        "    def build_model(ensemble_outputs,ensemble_visible,lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
        "        merge = concatenate(ensemble_outputs)\n",
        "        \n",
        "        hidden = Dense(12, activation='relu')(merge)\n",
        "        output = Dense(6, activation='sigmoid')(hidden)\n",
        "        model = Model(inputs=ensemble_visible, outputs=output)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "            \n",
        "        #model = Model(inputs = inp, outputs = x)\n",
        "        #model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "        X = [X_train for _ in range(2)]\n",
        "        \n",
        "        X_v = [X_val for _ in range(2)]\n",
        "        model.summary()\n",
        "        model.fit([np.asarray(X_train),np.asarray(X_train)],y_train, batch_size = 2048, epochs = 1, validation_data = ([np.asarray(X_val),np.asarray(X_val)], y_val),verbose = 1, callbacks = [ra_val, earlystop])\n",
        "            \n",
        "        return model\n",
        "                                         \n",
        "    modelx = build_model(ensemble_outputs,ensemble_visible,lr = 1e-3, lr_d = 0, units = 144, dr = 0.2)\n",
        "        \n",
        "    pred = modelx.predict([X_val,X_val], batch_size = 1024, verbose = 1)          \n",
        "        \n",
        "    oof[val_idx] = pred\n",
        "        \n",
        "    sub_preds += modelx.predict([X_te,X_te], batch_size=1024, verbose=1) / n_splits\n",
        "            \n",
        "auc=0\n",
        "for i in range(len(list_classes)):\n",
        "    auc += roc_auc_score(y[:,i], oof[:,i]) / len(list_classes)\n",
        "    \n",
        "print(\"AUC for full run: %.6f\" % auc)\n",
        "    \n",
        "validation = pd.DataFrame(oof, columns = list_classes)\n",
        "validation.to_csv('Ensemble', index=False) \n",
        "    \n",
        "submission = pd.concat([test['id'], pd.DataFrame(sub_preds, columns = list_classes)], axis=1)\n",
        "submission.to_csv('sEnsemble_Submission', index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold 0\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 150)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 150, 300)     81699000    input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           (None, 150)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_7 (SpatialDro (None, 150, 300)     0           embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 150, 200)     54466000    input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 150, 288)     384480      spatial_dropout1d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_11 (SpatialDr (None, 150, 200)     0           embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 148, 80)      69200       bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 148, 80)      69200       bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 148, 80)      69200       bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 148, 80)      69200       bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 150, 36)      7236        spatial_dropout1d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 149, 36)      14436       spatial_dropout1d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 148, 36)      21636       spatial_dropout1d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 146, 36)      36036       spatial_dropout1d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_25 (Global (None, 80)           0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_26 (Global (None, 80)           0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_27 (Global (None, 80)           0           conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_28 (Global (None, 80)           0           conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_26 (Gl (None, 80)           0           conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_27 (Gl (None, 80)           0           conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_25 (Gl (None, 80)           0           conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_28 (Gl (None, 80)           0           conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_33 (MaxPooling1D) (None, 1, 36)        0           conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_34 (MaxPooling1D) (None, 1, 36)        0           conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_35 (MaxPooling1D) (None, 1, 36)        0           conv1d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_36 (MaxPooling1D) (None, 1, 36)        0           conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 320)          0           global_max_pooling1d_25[0][0]    \n",
            "                                                                 global_max_pooling1d_26[0][0]    \n",
            "                                                                 global_max_pooling1d_27[0][0]    \n",
            "                                                                 global_max_pooling1d_28[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 320)          0           global_average_pooling1d_26[0][0]\n",
            "                                                                 global_average_pooling1d_27[0][0]\n",
            "                                                                 global_average_pooling1d_25[0][0]\n",
            "                                                                 global_average_pooling1d_28[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4, 36)        0           max_pooling1d_33[0][0]           \n",
            "                                                                 max_pooling1d_34[0][0]           \n",
            "                                                                 max_pooling1d_35[0][0]           \n",
            "                                                                 max_pooling1d_36[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 640)          0           concatenate_19[0][0]             \n",
            "                                                                 concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 144)          0           concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 640)          0           concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 144)          576         flatten_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 144)          92304       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 6)            870         batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 6)            870         dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 12)           0           dense_8[0][0]                    \n",
            "                                                                 dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 12)           156         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 6)            78          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 137,000,478\n",
            "Trainable params: 835,190\n",
            "Non-trainable params: 136,165,288\n",
            "__________________________________________________________________________________________________\n",
            "Train on 127656 samples, validate on 31915 samples\n",
            "Epoch 1/1\n",
            "127656/127656 [==============================] - 50s 395us/step - loss: 0.3852 - acc: 0.0040 - val_loss: 0.3710 - val_acc: 0.0036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-3afa114aa04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodelx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mensemble_visible\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-3afa114aa04f>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(ensemble_outputs, ensemble_visible, lr, lr_d, units, dr)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mX_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mra_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    222\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b65b6edfd920>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[    0,     0,     0, ...,  2086,    33,     9],\n       [    0,     0,     0, ...,   131,  1700, 70116],\n       [    0,     0,     0, ...,  3240,  6513,    28],\n       ...,\n       [    0,     ..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFfVLEulOkal",
        "colab_type": "code",
        "outputId": "48b0a51a-2846-4edc-a51c-5a84916ea366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y[:,0], y[:,0])): #StratifiedKFold expects array of shape (n,)\n",
        "        \n",
        "    X_train, y_train = X_t[trn_idx], y[trn_idx]\n",
        "    X_val, y_val = X_t[val_idx], y[val_idx]\n",
        "        \n",
        "    print(\"Running fold %d\" % fold_)     \n",
        "        \n",
        "    ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
        "    earlystop = EarlyStopping(monitor='val_loss', mode=\"min\", patience=5, verbose=1) \n",
        "        \n",
        "    def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
        "        merge = concatenate(ensemble_outputs)\n",
        "        hidden = Dense(10, activation='relu')(merge)\n",
        "        output = Dense(6, activation='sigmoid')(hidden)\n",
        "        model = Model(inputs=ensemble_visible, outputs=output)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "            \n",
        "        #model = Model(inputs = inp, outputs = x)\n",
        "        #model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "        X = [X_train for _ in range(len(model.input))]\n",
        "        model.fit(X, y_train, batch_size = 128, epochs = 4, validation_data = (X_val, y_val),verbose = 1, callbacks = [ra_val, earlystop])\n",
        "            \n",
        "        return model\n",
        "                                         \n",
        "    model = build_model(lr = 1e-3, lr_d = 0, units = 144, dr = 0.2)\n",
        "        \n",
        "    pred = model.predict(X_val, batch_size = 1024, verbose = 1)          \n",
        "        \n",
        "    oof[val_idx] = pred\n",
        "        \n",
        "    sub_preds += model.predict([X_te], batch_size=1024, verbose=1) / n_splits\n",
        "            \n",
        "auc=0\n",
        "for i in range(len(list_classes)):\n",
        "    auc += roc_auc_score(y[:,i], oof[:,i]) / len(list_classes)\n",
        "    \n",
        "print(\"AUC for full run: %.6f\" % auc)\n",
        "    \n",
        "validation = pd.DataFrame(oof, columns = list_classes)\n",
        "validation.to_csv('ensemble.csv', index=False) \n",
        "    \n",
        "submission = pd.concat([test['id'], pd.DataFrame(sub_preds, columns = list_classes)], axis=1)\n",
        "submission.to_csv('ensemble_val.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-526d8aa34610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-526d8aa34610>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(lr, lr_d, units, dr)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mra_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 4 array(s), but instead got the following list of 1 arrays: [array([[    0,     0,     0, ...,  2086,    33,     9],\n       [    0,     0,     0, ...,   131,  1700, 70116],\n       [  584,  6486,    49, ...,    60,     9,   584],\n       ...,\n       [    0,     ..."
          ]
        }
      ]
    }
  ]
}